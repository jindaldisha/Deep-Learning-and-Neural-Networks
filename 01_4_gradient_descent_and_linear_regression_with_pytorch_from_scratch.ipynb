{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_3_gradient_descent_and_linear_regression_with_pytorch_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrXLRTS5KJrYq8R1ACrQl3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jindaldisha/Deep-Learning-and-Neural-Networks/blob/main/00_3_gradient_descent_and_linear_regression_with_pytorch_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUBA3sipUh1R"
      },
      "source": [
        "#Gradient Descent and Linear Regression with PyTorch from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgaXn8O9SVX0"
      },
      "source": [
        "##Introduction to Linear Regression\n",
        "Linear Regression is one of the foundational algorithms in Machine Learning.\n",
        "\n",
        "We'll create a model that predicts crop yields for apples and oranges (target variables) by looking at the average temperature, rainfall, and humidity (input variables or features) in a region.\n",
        "\n",
        "Data:\n",
        "\n",
        "| Region | Temp (F) | Rainfall (mm) | Humidity (%) | Apples (ton) | Oranges (ton) |\n",
        "| -- | -- | -- | -- | -- | -- |\n",
        "| Kanto | 73 | 67 | 43 | 56 | 70 |\n",
        "| Johto | 91 | 88 | 64 | 81 | 101 | \n",
        "| Hoenn | 87 | 134 | 58 | 119 | 133 | \n",
        "| Sinnoh | 102 | 43 | 37 | 22 | 37 | \n",
        "| Unova | 69 | 96 | 70 | 103 | 119 |  \n",
        "\n",
        "In a linear regression model, each target variable is estimated to be a weighted (w) sum of the input variables, offset by some constant (b), known as a bias :\n",
        "\n",
        "`yield_apple = w11 * temp + w12 * rainfall + w13 * humidity + b1`\n",
        "\n",
        "`yield_oranges = w21 * temp + w22 * rainfall + w23 * humidity + b2`\n",
        "\n",
        "The learning part of linear regression is to figure out the weights (w11, w12..) and the biases (b1, b2..) using the training data and make accurate predictions for the test data that we pass to it. The learned weights will be used to predict the yields for apples and oranges in new regions using the average temperature, rainfall and humidity for that region.\n",
        "\n",
        "We train our model by adjusting the weights and biases slighty many time and keep improving our accuracy. To improve our model, we use an optimization technique called **Gradient Descent**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQZn5pTRWOfK"
      },
      "source": [
        "#Import Libraries\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHw_tTkFbsOG"
      },
      "source": [
        "##Training data\n",
        "\n",
        "We'll represent the training data using matrics x_train and y_train.\n",
        "\n",
        "x_train will have the values of temperature, rainfall and humidity for every region as a single row each.\n",
        "y_train will have the crop yields of apples and oranges for every region as a single row each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMfbPguIcEPe",
        "outputId": "f53af215-2e24-4ee7-c634-e50b689abc40"
      },
      "source": [
        "#Input Features (Temp, Rainfall, Humidity)\n",
        "x_train = np.array([[73, 67, 43], #Kanto\n",
        "                    [91, 88, 64], #Johto\n",
        "                    [87, 134, 58], #Hoenn\n",
        "                    [102, 43, 37], #Sinnoh\n",
        "                    [69, 96, 70]], #Unova\n",
        "                    dtype= 'float32')\n",
        "x_train"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 73.,  67.,  43.],\n",
              "       [ 91.,  88.,  64.],\n",
              "       [ 87., 134.,  58.],\n",
              "       [102.,  43.,  37.],\n",
              "       [ 69.,  96.,  70.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3emE20cEcjrl",
        "outputId": "80776a93-c6c1-432e-dad2-a2ae700e9392"
      },
      "source": [
        "# Input Labels (Apples, Oranges)\n",
        "y_train = np.array([[56, 70], \n",
        "                    [81, 101], \n",
        "                    [119, 133], \n",
        "                    [22, 37], \n",
        "                    [103, 119]], dtype='float32')\n",
        "y_train"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 56.,  70.],\n",
              "       [ 81., 101.],\n",
              "       [119., 133.],\n",
              "       [ 22.,  37.],\n",
              "       [103., 119.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElQKBU1Qc5ZX"
      },
      "source": [
        "#Convert input numpy array to pytorch tensors\n",
        "x_train = torch.from_numpy(x_train)\n",
        "y_train = torch.from_numpy(y_train)"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjU_q7CqdeKV",
        "outputId": "855a3258-9ab6-4369-d42c-6a29aca83a65"
      },
      "source": [
        "x_train, type(x_train), x_train.shape"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.]]), torch.Tensor, torch.Size([5, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3nbmqqMdeHe",
        "outputId": "6542329f-77bd-4e7f-e741-bc57bac686c6"
      },
      "source": [
        "y_train, type(y_train), y_train.shape"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 56.,  70.],\n",
              "         [ 81., 101.],\n",
              "         [119., 133.],\n",
              "         [ 22.,  37.],\n",
              "         [103., 119.]]), torch.Tensor, torch.Size([5, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATHKr-ZNdZIe"
      },
      "source": [
        "## Linear regression model from scratch\n",
        "\n",
        "The weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices, initialized as random values. The first row of `w` and the first element of `b` are used to predict the first target variable, i.e., yield of apples, and similarly, the second for oranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVg--i4FdXNd",
        "outputId": "08f26560-53ea-44c7-9f93-da303f9c0cee"
      },
      "source": [
        "w = torch.randn(2,3, requires_grad = True)\n",
        "b = torch.randn(2, requires_grad = True)\n",
        "w, b"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.5492, -0.3743,  0.9817],\n",
              "         [ 0.0911,  0.8713,  0.9355]], requires_grad=True),\n",
              " tensor([-0.5123,  1.4734], requires_grad=True))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j3lMK8ngbK7"
      },
      "source": [
        "Our Model is going to perform matrix multiplication of input features `x_train` and weights `w` (transposed) and add biases `b`.\n",
        "\n",
        "torch.randn creates a tensor with the given shape, with elements picked randomly from a normal distribution with mean 0 and standard deviation 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X8Qz-yZga7s"
      },
      "source": [
        "#Function for model\n",
        "def model(x):\n",
        "  return x @ w.t() + b"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK0iJVgVgRF8"
      },
      "source": [
        "#Calling the model on our input featurs\n",
        "y_pred = model(x_train)"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POr5oJ2qhCXU",
        "outputId": "bd5e3eb9-efbe-484d-93c7-ddbd5bf75b85"
      },
      "source": [
        "#Comparing y_pred with y_train\n",
        "y_pred, y_train"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-23.4729, 106.7221],\n",
              "         [-20.6040, 146.3027],\n",
              "         [-41.5157, 180.4041],\n",
              "         [-36.3078,  82.8404],\n",
              "         [ -5.6249, 156.8817]], grad_fn=<AddBackward0>), tensor([[ 56.,  70.],\n",
              "         [ 81., 101.],\n",
              "         [119., 133.],\n",
              "         [ 22.,  37.],\n",
              "         [103., 119.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6MTTAVwhNTW"
      },
      "source": [
        "There is a big difference between our model's predicted labels and the true labels. This is because we've initializes our model with random weights and biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa44aQRQheoW"
      },
      "source": [
        "##Loss Function\n",
        "\n",
        "Before we make changes in our model to improve our predictions, we need a way to evaluate our model. We can compare the predicted labels with true labels using the following steps\"\n",
        "- Calculate the difference between `y_pred` and `y_train`\n",
        "- Take a square of all the differences\n",
        "- Calculate the average of the elements.\n",
        "\n",
        "The result is a single numbers and this is know as the `mean square error (MSE)`. It is mean of the square of the difference in the predicted value and the true value.\n",
        "\n",
        "On average, each element in the prediction differs from the actual target by the square root of the loss. The result is called the loss because it indicates how bad the model is at predicting the target variables. It represents information loss in the model: the lower the loss, the better the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW5UE7QQhLom"
      },
      "source": [
        "#Loss Function (MSE)\n",
        "def mse(y_true, y_pred):\n",
        "  error = y_true - y_pred\n",
        "  square = error * error\n",
        "  number_of_elements = error.numel()\n",
        "  mean = torch.sum(square) / number_of_elements\n",
        "  return mean"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODu6S-Cwi5Tz",
        "outputId": "2444811e-adfb-4220-95b4-9cfb62ad327c"
      },
      "source": [
        "#Compute Loss\n",
        "loss = mse(y_train, y_pred)\n",
        "print(loss)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6678.8149, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O25igopQj83C"
      },
      "source": [
        "## Compute gradients\n",
        "\n",
        "With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases because they have `requires_grad` set to `True`.\n",
        "\n",
        "The gradients are stored in the `.grad` property of the respective tensors. Note that the derivative of the loss w.r.t. the weights matrix is itself a matrix with the same dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGVDgWh_j8nO"
      },
      "source": [
        "#Compute Gradients\n",
        "loss.backward()"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChGQKmp7j8kJ",
        "outputId": "6b60d4d5-c7f7-49ab-d2b7-33180d1923ea"
      },
      "source": [
        "#Gradients for weights\n",
        "print(w)\n",
        "print(w.grad) "
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5492, -0.3743,  0.9817],\n",
            "        [ 0.0911,  0.8713,  0.9355]], requires_grad=True)\n",
            "tensor([[-8490.9727, -9742.0332, -5798.2070],\n",
            "        [ 3643.3960,  3681.3899,  2315.1355]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weqOLHTunGis"
      },
      "source": [
        "Since we've calculated gradients for the loss.\n",
        "`w.grad` is the derivative of the `loss` w.r.t to `w`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZybyboxY77RW",
        "outputId": "61239896-0c10-4770-d537-85e50b552f2c"
      },
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.5123,  1.4734], requires_grad=True)\n",
            "tensor([-101.7051,   42.6302])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efZRn2VWnWOJ"
      },
      "source": [
        "## Adjust Weights and Biases to reduce the Loss\n",
        "\n",
        "The loss is function of our weights and biases and we want to find the set of weights where the loss is the lowest. \n",
        "\n",
        "The gradients indicate the rate of change of the loss, i.e. the loss function's slope w.r.t the weights and biases.\n",
        "\n",
        "If the gradient element is positive:\n",
        " - Increasing the weight element's value slightly will increase the loss.\n",
        " - Decreasing the weight element's value will decrease the loss.\n",
        "\n",
        "If the gradient element is negative:\n",
        " - Increasing the weight element's value slightly will decrease the loss.\n",
        " - Decreasing the weight element's value will increase the loss.\n",
        "\n",
        "\n",
        "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss w.r.t. that element. This observation forms the basis of the gradient descent optimization algorithm that we'll use to improve our model (by descending along the gradient).\n",
        "\n",
        "We can subtract from each weight element a small quantity proportional to the derivative of the loss w.r.t. that element to reduce the loss slightly.\n",
        "\n",
        "\n",
        "In simple words, if the gradient is negative, the slope is negative so we need to increase the weight. If the gradient is positive, the slope is positive so we need to decrease the weight.\n",
        "\n",
        "It is called gradient descent, because we're descending along the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-1Wt_pGnF7Y"
      },
      "source": [
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twhg6Rd43-Mg"
      },
      "source": [
        "We multiply the gradients with a very small number to ensure that we don't modify the weights by a very large amount. This ensure that we only take small steps in the downhill direction and not giant leaps. This prevents from diverging away from the optimal solution. This number is known as the learning rate. \n",
        "We use torch.no_grad to we shouldn't track, calculate or modify the gradients while updating the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOQL1CTCi_4l",
        "outputId": "62dfaa77-cc66-4046-8395-76e7cba269a3"
      },
      "source": [
        "#Evaluate the model again\n",
        "y_pred = model(x_train)\n",
        "loss = mse(y_train, y_pred)\n",
        "print(loss)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4558.9028, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I0tY--p7oAa"
      },
      "source": [
        "Before we proceed, we reset the gradients to zero by invoking the `.zero_()` method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke `.backward` on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZRsh0HN7Jfk",
        "outputId": "fadc988d-eddb-421c-ef1f-20eddc6aae67"
      },
      "source": [
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUAI7Bc87q6q"
      },
      "source": [
        "## Train the model using gradient descent\n",
        "\n",
        "We reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can train the model using the following steps:\n",
        "\n",
        "1. Generate predictions\n",
        "\n",
        "2. Calculate the loss\n",
        "\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "\n",
        "5. Reset the gradients to zero\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdFt67ct8Zmm",
        "outputId": "7441d51e-1785-4c9f-c391-56b7efc414ab"
      },
      "source": [
        "# Step 1: Generate Predictions\n",
        "y_pred = model(x_train)\n",
        "y_pred"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -8.2531, 100.6000],\n",
              "        [ -0.5924, 138.2655],\n",
              "        [-17.7103, 170.9580],\n",
              "        [-21.3115,  76.6842],\n",
              "        [ 13.6460, 149.2126]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWiSXcHW9muy",
        "outputId": "42a7b8e7-9626-4b56-be28-d54b32454ea9"
      },
      "source": [
        "# Step 2: Calculate Loss\n",
        "loss = mse(y_pred, y_train)\n",
        "loss"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4558.9028, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWUgQ8Yj9vyc"
      },
      "source": [
        "#Step 3:Compute Gradients\n",
        "loss.backward()"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yImgOwPS94en",
        "outputId": "6fbea757-4bc4-4373-bae5-d6c5c7b2d11d"
      },
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-6918.4756, -8048.9282, -4754.2598],\n",
            "        [ 3011.9531,  3004.5544,  1897.1111]])\n",
            "tensor([-83.0443,  35.1441])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Ja90Oc97wa"
      },
      "source": [
        "#Step 4, 5: Adjust Weights and Reset Gradients\n",
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca7atXMN-Np4",
        "outputId": "24f520b8-8b4b-4591-d9bc-62f52c15a3c1"
      },
      "source": [
        "#View new weights and biases\n",
        "w, b"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.3952, -0.1964,  1.0872],\n",
              "         [ 0.0245,  0.8044,  0.8933]], requires_grad=True),\n",
              " tensor([-0.5104,  1.4727], requires_grad=True))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv5vGwn9-R-m",
        "outputId": "62e90ddf-ebb1-40db-9ced-8ef3848651ce"
      },
      "source": [
        "#Evaluate the Model again\n",
        "y_pred = model(x_train)\n",
        "loss = mse(y_train, y_pred)\n",
        "print(loss)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3129.6816, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybgY_YrT-ltT"
      },
      "source": [
        "##Train for multiple epochs\n",
        "\n",
        "To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradient descent algorithm multiple times. Each iteration is called an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUO610zW-_eO",
        "outputId": "013ebde1-f195-43fb-dd4c-82cfa139e217"
      },
      "source": [
        "#Train for 100 epochs\n",
        "for i in range(100):\n",
        "  #Make Prediction\n",
        "  y_pred = model(x_train)\n",
        "  #Calculate Loss\n",
        "  loss = mse(y_train, y_pred)\n",
        "  #Print Loss at each epoch\n",
        "  print(f'Epoch [{i+1}/100], Loss: {loss}')\n",
        "  #Calculate Gradients\n",
        "  loss.backward()\n",
        "  #Gradient Descent\n",
        "  with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    #Reset gradients to zero\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 3129.681640625\n",
            "Epoch [2/100], Loss: 2165.91455078125\n",
            "Epoch [3/100], Loss: 1515.819580078125\n",
            "Epoch [4/100], Loss: 1077.10986328125\n",
            "Epoch [5/100], Loss: 780.858154296875\n",
            "Epoch [6/100], Loss: 580.6143188476562\n",
            "Epoch [7/100], Loss: 445.07611083984375\n",
            "Epoch [8/100], Loss: 353.14971923828125\n",
            "Epoch [9/100], Loss: 290.61968994140625\n",
            "Epoch [10/100], Loss: 247.90695190429688\n",
            "Epoch [11/100], Loss: 218.5556182861328\n",
            "Epoch [12/100], Loss: 198.2154083251953\n",
            "Epoch [13/100], Loss: 183.95474243164062\n",
            "Epoch [14/100], Loss: 173.79769897460938\n",
            "Epoch [15/100], Loss: 166.41268920898438\n",
            "Epoch [16/100], Loss: 160.90249633789062\n",
            "Epoch [17/100], Loss: 156.66229248046875\n",
            "Epoch [18/100], Loss: 153.28428649902344\n",
            "Epoch [19/100], Loss: 150.493896484375\n",
            "Epoch [20/100], Loss: 148.10568237304688\n",
            "Epoch [21/100], Loss: 145.9948272705078\n",
            "Epoch [22/100], Loss: 144.07696533203125\n",
            "Epoch [23/100], Loss: 142.2953338623047\n",
            "Epoch [24/100], Loss: 140.61154174804688\n",
            "Epoch [25/100], Loss: 138.99951171875\n",
            "Epoch [26/100], Loss: 137.4417724609375\n",
            "Epoch [27/100], Loss: 135.92640686035156\n",
            "Epoch [28/100], Loss: 134.44532775878906\n",
            "Epoch [29/100], Loss: 132.99305725097656\n",
            "Epoch [30/100], Loss: 131.56568908691406\n",
            "Epoch [31/100], Loss: 130.16061401367188\n",
            "Epoch [32/100], Loss: 128.77603149414062\n",
            "Epoch [33/100], Loss: 127.41064453125\n",
            "Epoch [34/100], Loss: 126.0634994506836\n",
            "Epoch [35/100], Loss: 124.73380279541016\n",
            "Epoch [36/100], Loss: 123.421142578125\n",
            "Epoch [37/100], Loss: 122.125\n",
            "Epoch [38/100], Loss: 120.8450927734375\n",
            "Epoch [39/100], Loss: 119.5810317993164\n",
            "Epoch [40/100], Loss: 118.33255767822266\n",
            "Epoch [41/100], Loss: 117.09952545166016\n",
            "Epoch [42/100], Loss: 115.88166809082031\n",
            "Epoch [43/100], Loss: 114.6786880493164\n",
            "Epoch [44/100], Loss: 113.49049377441406\n",
            "Epoch [45/100], Loss: 112.31681823730469\n",
            "Epoch [46/100], Loss: 111.15757751464844\n",
            "Epoch [47/100], Loss: 110.01243591308594\n",
            "Epoch [48/100], Loss: 108.88130950927734\n",
            "Epoch [49/100], Loss: 107.7640151977539\n",
            "Epoch [50/100], Loss: 106.66035461425781\n",
            "Epoch [51/100], Loss: 105.57017517089844\n",
            "Epoch [52/100], Loss: 104.49330139160156\n",
            "Epoch [53/100], Loss: 103.42955017089844\n",
            "Epoch [54/100], Loss: 102.3787612915039\n",
            "Epoch [55/100], Loss: 101.34080505371094\n",
            "Epoch [56/100], Loss: 100.31546783447266\n",
            "Epoch [57/100], Loss: 99.30264282226562\n",
            "Epoch [58/100], Loss: 98.30213165283203\n",
            "Epoch [59/100], Loss: 97.31376647949219\n",
            "Epoch [60/100], Loss: 96.33747100830078\n",
            "Epoch [61/100], Loss: 95.37300109863281\n",
            "Epoch [62/100], Loss: 94.4202651977539\n",
            "Epoch [63/100], Loss: 93.47911071777344\n",
            "Epoch [64/100], Loss: 92.54939270019531\n",
            "Epoch [65/100], Loss: 91.6309585571289\n",
            "Epoch [66/100], Loss: 90.72364807128906\n",
            "Epoch [67/100], Loss: 89.82734680175781\n",
            "Epoch [68/100], Loss: 88.94194793701172\n",
            "Epoch [69/100], Loss: 88.06725311279297\n",
            "Epoch [70/100], Loss: 87.2031478881836\n",
            "Epoch [71/100], Loss: 86.34953308105469\n",
            "Epoch [72/100], Loss: 85.50617980957031\n",
            "Epoch [73/100], Loss: 84.67308044433594\n",
            "Epoch [74/100], Loss: 83.85002136230469\n",
            "Epoch [75/100], Loss: 83.03691101074219\n",
            "Epoch [76/100], Loss: 82.2336654663086\n",
            "Epoch [77/100], Loss: 81.4400863647461\n",
            "Epoch [78/100], Loss: 80.65609741210938\n",
            "Epoch [79/100], Loss: 79.88154602050781\n",
            "Epoch [80/100], Loss: 79.11637115478516\n",
            "Epoch [81/100], Loss: 78.36038208007812\n",
            "Epoch [82/100], Loss: 77.61349487304688\n",
            "Epoch [83/100], Loss: 76.87561798095703\n",
            "Epoch [84/100], Loss: 76.14659881591797\n",
            "Epoch [85/100], Loss: 75.42636108398438\n",
            "Epoch [86/100], Loss: 74.71479034423828\n",
            "Epoch [87/100], Loss: 74.01175689697266\n",
            "Epoch [88/100], Loss: 73.31717681884766\n",
            "Epoch [89/100], Loss: 72.63089752197266\n",
            "Epoch [90/100], Loss: 71.9529037475586\n",
            "Epoch [91/100], Loss: 71.28299713134766\n",
            "Epoch [92/100], Loss: 70.62113189697266\n",
            "Epoch [93/100], Loss: 69.96717834472656\n",
            "Epoch [94/100], Loss: 69.3210678100586\n",
            "Epoch [95/100], Loss: 68.68268585205078\n",
            "Epoch [96/100], Loss: 68.05194854736328\n",
            "Epoch [97/100], Loss: 67.42872619628906\n",
            "Epoch [98/100], Loss: 66.81295013427734\n",
            "Epoch [99/100], Loss: 66.2044906616211\n",
            "Epoch [100/100], Loss: 65.60334777832031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR_a2g7SAMIP"
      },
      "source": [
        "As it can be seen from above, the loss decreases with each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSqTAzVkAL0h",
        "outputId": "e764fc89-3a28-4e1d-d1d0-88a685337c6e"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 59.2773,  71.6623],\n",
              "        [ 88.9440, 100.8018],\n",
              "        [ 99.9737, 130.4883],\n",
              "        [ 34.3872,  43.3723],\n",
              "        [105.7756, 115.9469]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG-4GLxrA5gN",
        "outputId": "a57d97e6-ee57-4f0f-9005-231e569ce88a"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_VZsHtfAhIX"
      },
      "source": [
        "The predictions are quite close to our targets. We have a trained a reasonably good model to predict crop yields for apples and oranges by looking at the average temperature, rainfall, and humidity in a region. We can use it to make predictions of crop yields for new regions by passing a batch containing a single row of input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqs98G9H_wj2"
      },
      "source": [
        "The approach in machine learning is very different from classical programming. Usually we write programs that take some inputs, perform some operations and return the result. \n",
        "However, here we've defined a 'mode' that assumes a specific relation between the inputs and outputs, expresses using some random parameters i.e. weights and biases. We then show the model some known inputs and outputs and train the model to come up with good values for the unknown parameters. Once trained the model can be used to compute the outputs for new inputs.Deep learning is a branch of machine learning that uses matrix operations, non-linear activation functions and gradient descent to build and train models."
      ]
    }
  ]
}
